{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "from utils import SQLHandler\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import log_loss, roc_auc_score, recall_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "handler = SQLHandler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "select *\n",
    "from data_hb.e_jir_sample_0830_zyf\n",
    "\"\"\"\n",
    "\n",
    "df = handler.query(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql =\"\"\"\n",
    "select *\n",
    "from data_hb.e_jir_sample_0830_appList\n",
    "\n",
    "\"\"\"\n",
    "df_app = handler.query(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "select *\n",
    "from data_hb.e_jir_sample_0830_xd\n",
    "\n",
    "\"\"\"\n",
    "df_xd = handler.query(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "SELECT *\n",
    "from\n",
    "data_hb.e_jir_sample_0830_location\n",
    "  \"\"\"\n",
    "df_location = handler.query(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "select *\n",
    "from\n",
    "(select *, row_number() over(partition by uid order by date desc) as r\n",
    "from data_hb.e_jir_sample_0830_xd\n",
    ")t\n",
    "where r = 1\n",
    "\"\"\"\n",
    "df_xd2 = handler.query(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "select *\n",
    "from\n",
    "data_hb.e_jir_sample_0830_act_province\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "df_ac_city = handler.query(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "select *\n",
    "from \n",
    "data_hb.e_jirong_sample_0830_f2\n",
    "\"\"\"\n",
    "\n",
    "df_f2 = handler.query(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据处理，特征衍生"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 申请记录数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 两类小贷产品xd1, xd2。 获取申请前90天这两类小贷产品的多头和占比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_xd_s = pd.DataFrame(df_xd.groupby(['uid','p_label'])['level'].count()).reset_index()\n",
    "df_xd_s = pd.pivot(df_xd_s, index = 'uid', columns = 'p_label', values = 'level')\n",
    "df_xd_s.fillna(0,inplace=True)\n",
    "df_xd_s.reset_index(inplace=True)\n",
    "df_xd_s = df_xd_s.merge(df_xd2[['uid','year','level','p_operate','phone_model','date','bl_flag','ex_flag','soc_flag']].drop_duplicates('uid'),on = 'uid')\n",
    "df_xd_s.fillna(-1,inplace=True)\n",
    "df_xd_s['xd1_per'] = df_xd_s['xd1_flag'] / (df_xd_s['xd1_flag'] + df_xd_s['xd2_flag'] )\n",
    "\n",
    "\n",
    "#合并到主表\n",
    "df = df[['uid','age','level','p_operate','phone_model']].merge(df_xd_s[['uid','xd1_flag','xd2_flag','bl_flag','ex_flag','soc_flag','xd1_per']],how='left',on='uid')\n",
    "df[['xd1_flag','xd2_flag']] = df[['xd1_flag','xd2_flag']].fillna(0)\n",
    "df.fillna(-1,inplace=True)\n",
    "\n",
    "\n",
    "df = df.merge(df_xd2[['uid','p_label']], on ='uid',how = 'left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 地理位置数据，用城市等级编码，加入活跃城市和活跃指数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_rank = \"\"\"北京、上海、广州、深圳\n",
    "成都、重庆、杭州、武汉、西安、南京、天津、苏州、长沙、沈阳、青岛、济南、合肥、东莞、昆明、福州、无锡、厦门、哈尔滨、长春、南昌、济南、宁波、大连、贵阳、温州、石家庄、泉州、南宁、金华、常州、珠海、惠州、嘉兴、南通、中山、保定、兰州、台州、徐州、太原、绍兴、烟台、廊坊\n",
    "海口、汕头、潍坊、扬州、洛阳、乌鲁木齐、临沂、唐山、镇江、盐城、湖州、赣州、漳州、揭阳、江门、桂林、邯郸、泰州、济宁、呼和浩特、咸阳、芜湖、三亚、阜阳、淮安 遵义、银川、衡阳、上饶、柳州、淄博、莆田、绵阳、湛江、商丘、宜昌、沧州、连云港、南阳、蚌埠、驻马店、滁州、邢台、潮州、秦皇岛、肇庆、荆州、周口\"\"\"\n",
    "\n",
    "\n",
    "city_l = city_rank.split('\\n')\n",
    "l = []\n",
    "for i in city_l:\n",
    "    l.append(i.split('、'))\n",
    "di = {}\n",
    "for i in l[0]:\n",
    "    di[i] = \"一线\"\n",
    "for i in l[1]:\n",
    "    di[i] = \"二线\"\n",
    "for i in l[2]:\n",
    "    di[i] = \"三线\"\n",
    "    \n",
    "df_location['city_rank'] = df_location['city'].map(di)\n",
    "df_location.fillna('四线',inplace=True)    \n",
    "\n",
    "df = df.merge(df_location[['uid','city_rank']],how = 'left',on = 'uid')\n",
    "\n",
    "#活跃城市\n",
    "df= df.merge(df_ac_city[['uid','rate_same_province','max_rate_act_province']], on ='uid', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 其他数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#这张表包含小贷产品注册数，信用卡持卡数，小微企业主标签，信用卡申请标签\n",
    "df = df.merge(df_f2, on ='uid',how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## app列表特征衍生"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#基于bad rate计算每个app的tf-idf。 tf为 app的未通过人数 / 总未通过人数； idf为 log(总人数 / 安装app的人数)\n",
    "\n",
    "total_user = 56956 + 32533\n",
    "bad_user = 56956\n",
    "df_app3 = df_app.merge(df[['uid','label']],on='uid')\n",
    "df_app_bad = pd.DataFrame(df_app3.groupby('brand')['label'].sum())\n",
    "df_app_bad.columns=['bad_count']\n",
    "df_app_bad['tf'] = df_app_bad['bad_count'] / bad_user\n",
    "df_app_all = pd.DataFrame(df_app3.groupby('brand')['label'].count())\n",
    "df_app_bad = df_app_bad.merge(df_app_all,on = 'brand')\n",
    "df_app_bad['idf'] = np.log(total_user / df_app_bad['label'])\n",
    "df_app_bad['tf-idf'] = df_app_bad['tf'] * df_app_bad['idf']\n",
    "\n",
    "\n",
    "tfidf_dict = df_app_bad['tf-idf'].to_dict()\n",
    "df_app3['tfidf'] = df_app3['brand'].map(tfidf_dict)\n",
    "df_tfidf = pd.DataFrame(df_app3.groupby('uid')['tfidf'].apply(list))\n",
    "df_tfidf = df_tfidf.reset_index().merge(df,on='uid')\n",
    "\n",
    "\n",
    "#计算 1: 每个用户安装的app的平均tfidf； 2：每个用户安装app中tfidf大于0.25的数量\n",
    "df_tfidf['score_mean'] = df_tfidf['tfidf'].apply(lambda x: np.mean(x))\n",
    "def countRiskApp(x):\n",
    "    c = 0\n",
    "    for i in x:\n",
    "        if i >= 0.25:\n",
    "            c+=1\n",
    "    return c\n",
    "df_tfidf['risk_count'] = df_tfidf['tfidf'].apply(countRiskApp)\n",
    "df = df.merge(df_tfidf[['uid','score_mean','risk_count']], on = 'uid', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征编码， 加入app_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_app['brand'] = le.fit_transform(df_app['brand'])\n",
    "\n",
    "\n",
    "df_app2 = pd.DataFrame(df_app.groupby('uid')['brand'].apply(list))\n",
    "df_app2.reset_index(inplace=True)\n",
    "df = df.merge(df_app2[['uid','brand']], how = 'left',on = 'uid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df.apply(lambda x: 0 if (x['level'] == 'A') or (x['level'] =='B') else 1,axis = 1)\n",
    "df['age'] = 2023 - df['year'].fillna(-1).astype(int)\n",
    "df['age'] = df['age'].replace(2024,df['age'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cate_features_names = ['p_operate','phone_model','bl_flag','ex_flag','soc_flag','city_rank','p_label','busi','card']\n",
    "num_features_names = ['age','rate_same_province','max_rate_act_province','registered_product_count','dt_num','score_mean','risk_count','xd1_flag','xd2_flag','xd1_per']\n",
    "for f in cate_features_names:\n",
    "    le = LabelEncoder()\n",
    "    df[f] = df[f].astype('str')\n",
    "    df[f] = le.fit_transform(df[f])\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = df.drop(['label'],axis=1)\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# app列表特征衍生2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对app列表做两种衍生 1：one-hot编码，维度为(#user, #total_app); 2.app做label-encoding形成列表， 维度(#user, 用户有的最多app数量)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coo_indexes(lil):\n",
    "    rows=[]\n",
    "    cols=[]\n",
    "    for i,el in enumerate(lil):\n",
    "        if type(el) != list:\n",
    "            el = [el]\n",
    "            \n",
    "        for j in el:\n",
    "            rows.append(i)\n",
    "            cols.append(j)\n",
    "    return rows, cols\n",
    "\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "def get_sparse_features(series, shape):\n",
    "    coo_indexes = get_coo_indexes(series.tolist())\n",
    "    sparse_df = coo_matrix((np.ones(len(coo_indexes[0])), (coo_indexes[0], coo_indexes[1])),shape=shape)\n",
    "    \n",
    "    return sparse_df\n",
    "def sparse_to_idx(df, pad_idx = -1):\n",
    "    indexes = df.nonzero()\n",
    "    indexes_df = pd.DataFrame()\n",
    "    indexes_df['rows'] = indexes[0]\n",
    "    indexes_df['cols'] = indexes[1]\n",
    "    mdf = indexes_df.groupby('rows').apply(lambda x: x['cols'].tolist())\n",
    "    max_len = mdf.apply(lambda x: len(x)).max()\n",
    "    return mdf.apply(lambda x: pd.Series(x +[pad_idx] * (max_len - len(x)))).values\n",
    "\n",
    "prev_brand_train = get_sparse_features(X_train['brand'], (len(df),df_app['brand'].max()+1))\n",
    "prev_brand_test = get_sparse_features(X_test['brand'], (len(df),df_app['brand'].max()+1))\n",
    "\n",
    "brand_train_tensor = torch.sparse_coo_tensor(indices = prev_brand_train.nonzero(),\n",
    "                                            values = [1] * len(prev_brand_train.nonzero()[0]),\n",
    "                                            size = prev_brand_train.shape).to_dense()\n",
    "\n",
    "\n",
    "brand_test_tensor = torch.sparse_coo_tensor(indices = prev_brand_test.nonzero(),\n",
    "                                            values = [1] * len(prev_brand_test.nonzero()[0]),\n",
    "                                            size = prev_brand_test.shape).to_dense()\n",
    "\n",
    "brand_idx_train = torch.Tensor(sparse_to_idx(prev_brand_train,0)).long()\n",
    "brand_idx_test = torch.Tensor(sparse_to_idx(prev_brand_test,0)).long()\n",
    "\n",
    "target_train = torch.Tensor(y_train.values).long()\n",
    "target_valid = torch.Tensor(y_test.values).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class build_dataset(Dataset):\n",
    "    def __init__(self, dataf, cate_feature_names, num_feature_names, brand_tensor, brand_idx,target):\n",
    "        self.df = dataf\n",
    "        self.cate_feature_names = cate_feature_names\n",
    "        self.num_feature_names = num_feature_names\n",
    "        self.brand_tensor = brand_tensor\n",
    "        self.brand_idx = brand_idx\n",
    "        self.target = target\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        idx2 = [idx]\n",
    "        #print(self.num_feature_names)\n",
    "        #print(self.df.iloc[idx2][self.num_feature_names])\n",
    "        continuous_features = self.df.iloc[idx2][self.num_feature_names].values\n",
    "       \n",
    "        cate_features = self.df.iloc[idx2][self.cate_feature_names].values\n",
    "        \n",
    "        brand_tensor = self.brand_tensor[idx]\n",
    "        brand_idx = self.brand_idx[idx]\n",
    "        label = self.target[idx]\n",
    "        \n",
    "        return  torch.from_numpy(continuous_features), torch.from_numpy(cate_features), torch.tensor(brand_tensor),torch.tensor(brand_idx), label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = build_dataset(X_train, cate_features_names, num_features_names, brand_train_tensor,brand_idx_train, target_train)\n",
    "train_loader = DataLoader(dataset = train_dataset, batch_size= 128, shuffle = True)\n",
    "\n",
    "valid_dataset = build_dataset(X_test, cate_features_names, num_features_names, brand_test_tensor,brand_idx_test, target_valid)\n",
    "valid_loader = DataLoader(dataset = valid_dataset, batch_size= 128, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cate_vocab_size = []\n",
    "for i in cate_features_names:\n",
    "    cate_vocab_size.append(df[i].nunique())\n",
    "print(cate_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 改进wide and deep 模型， 使用两个deep层分开训练。 app_index列表单独训练。 wide层为app_onehot，为了记忆app之间的直接关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cat, mean\n",
    "class WideAndDeep(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        #continuous_feature_shape,\n",
    "        embed_size, # size of embedding for binary features\n",
    "        embed_dict_len,\n",
    "        cate_vocab_size,# number of unique binary features\n",
    "        num_size,\n",
    "        pad_idx # padding index\n",
    "    ):\n",
    "        super(WideAndDeep, self).__init__()\n",
    "        self.embed = nn.Embedding(embed_dict_len, embed_size, padding_idx=pad_idx)\n",
    "        self.embed_layer_list = []\n",
    "        for i in cate_vocab_size:\n",
    "            self.embed_layer_list.append(nn.Embedding(i,embed_size))\n",
    "        \n",
    "        self.embed_layer_list = nn.ModuleList(self.embed_layer_list)\n",
    "       \n",
    "        self.deep1 = nn.Sequential(\n",
    "            nn.Linear(embed_size * len(cate_vocab_size) + num_size, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),  \n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU()\n",
    "          \n",
    "        )\n",
    "        \n",
    "        self.deep2 = nn.Sequential(\n",
    "            nn.Linear(embed_size, 512),\n",
    "            nn.ReLU(),  \n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.wide = nn.Sequential(   \n",
    "            nn.Linear(embed_dict_len + 512, 1)\n",
    "        )\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, binary, binary_idx, cate_features, num_features):\n",
    "        \n",
    "        emb_out_list = []\n",
    "        cate_features = cate_features.squeeze()\n",
    "        num_features = num_features.squeeze()\n",
    "        \n",
    "        for i, emb_layer in enumerate(self.embed_layer_list):\n",
    "            \n",
    "            embed_out = emb_layer(cate_features[:,i].long())\n",
    "            emb_out_list.append(embed_out)\n",
    "            \n",
    "        cate = torch.cat(emb_out_list, dim=1)\n",
    "      \n",
    "        binary_embed = self.embed(binary_idx)\n",
    "        binary_embed_mean = mean(binary_embed, dim=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        x = torch.cat((cate, num_features),dim=1)\n",
    "        \n",
    "        deep1 = self.deep1(torch.tensor(x,dtype=torch.float32))\n",
    "        \n",
    "        deep2 = self.deep2(torch.tensor(binary_embed_mean))\n",
    "\n",
    "        total_logits = self.wide(torch.cat((deep1, deep2, binary),dim=1))\n",
    "        \n",
    "      \n",
    "\n",
    "        output = self.sigmoid(total_logits)\n",
    "        \n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "model = WideAndDeep(8,df_app['brand'].nunique(),cate_vocab_size,len(num_features_names), 0).to(device)\n",
    "loss_fcn = nn.BCELoss()\n",
    "#loss_fcn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.05, weight_decay = 0.001)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[3,8,15,20,30,40], gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_auc = 0\n",
    "epochs = 40\n",
    "for e in range(epochs):\n",
    "    train_labels = []\n",
    "    train_preds = []\n",
    "\n",
    "    train_loss_sum = 0.0\n",
    "    current_lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    print(f'Current lr: {current_lr}')\n",
    "    for idx, x in enumerate(train_loader):\n",
    "        model.train()       \n",
    "\n",
    "        num_features, cate_features, brand_train_tensor, brand_train_idx, label = x[0].to(device), x[1].to(device), x[2].to(device), x[3].to(device), x[4].to(device)\n",
    "\n",
    "        pred = model(brand_train_tensor, brand_train_idx, cate_features, num_features)\n",
    "        \n",
    "    \n",
    "        loss = loss_fcn(pred.squeeze().to(torch.float32), label.squeeze().to(torch.float32))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        train_labels.extend(label.squeeze().cpu().clone().detach().numpy())\n",
    "        train_preds.extend(pred.squeeze().cpu().clone().detach().numpy())\n",
    "        \n",
    "      \n",
    "\n",
    "        train_loss_sum += loss.cpu().item()\n",
    "        if(idx+1) % 20 ==0:\n",
    "            print(f\"loss: {train_loss_sum / (idx + 1)}\")\n",
    "            print(f\"train auc: {roc_auc_score(train_labels, train_preds)}\")\n",
    "            \n",
    "    scheduler.step()\n",
    "    print('eval--------------------------')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        valid_labels = []\n",
    "        valid_preds = []\n",
    "        for idx, x in enumerate(valid_loader):\n",
    "            model.train()       \n",
    "\n",
    "            num_features, cate_features, brand_train_tensor, brand_train_idx, label = x[0].to(device), x[1].to(device), x[2].to(device), x[3].to(device), x[4].to(device)\n",
    "\n",
    "            pred = model(brand_train_tensor, brand_train_idx, cate_features, num_features)\n",
    "\n",
    "            valid_labels.extend(label.squeeze().cpu().detach().numpy())\n",
    "            valid_preds.extend(pred.squeeze().cpu().detach().numpy())\n",
    "\n",
    "        auc = roc_auc_score(valid_labels, valid_preds)\n",
    "        print(f\"auc: {auc}\")\n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            print(f'model_saved,auc：{auc}')\n",
    "            torch.save(model.state_dict(),'model_credit_v4.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
